from scipy.sparse import dok_matrix
import multiprocessing
from nltk import ngrams
import os
import pickle
from scipy.sparse import load_npz, save_npz, vstack
import gc

path='D:\\Downloads\\'
files=os.listdir(path+'textFiles\\')
def bigrams(worker, files, f_dict):
    features=set([str(hex(x)).upper().lstrip('0X').rjust(4, '0') for x in range(2**16)])
    for file in files:
        with open(path+'textFiles\\'+file, 'r') as f:
            feature_counts={}
            for line in f.readlines():
                l="".join(line.split())
                for ng in ngrams(l, 4):
                    n="".join(ng)
                    if n in features:
                        try:
                            feature_counts[n]+=1
                        except KeyError:
                            feature_counts[n]=1
            f_dict[file.split('.')[0]]=feature_counts
    print(f'worker {worker} completed')

def main():
    manager=multiprocessing.Manager()
    f_dict=manager.dict()
    
    jobs=[]
    for pblk in range(18, 19, 1):
        p=multiprocessing.Process(target=bigrams, args=(pblk, files[pblk*603:(pblk+1)*603], f_dict))
        jobs.append(p)
        p.start()
    
    for job in jobs:
        job.join()
    
    
    file_ids=f_dict.keys()
    
    csr_vectors=[]
    i=0
    for file_id in file_ids:
        feat_vec=dok_matrix((1, 65536), dtype=float)
        bigram_dict=f_dict[file_id]
        b_list=bigram_dict.keys()
        for b in b_list:
            col=int(b, 16)
            feat_vec[0, col]=bigram_dict[b]
        csr_vectors.append(feat_vec.tocsr())
        feat_vec=None
        gc.collect()
        

    final_vec=csr_vectors[0]
    for vec in csr_vectors[1:]:
        final_vec=vstack((final_vec, vec))
        
    save_npz(path+'bigrams_7.npz', final_vec)
    
    with open(path + 'file_ids_7.pickle', 'wb') as fp:
        pickle.dump(file_ids, fp)
    
if __name__=='__main__':
    main()